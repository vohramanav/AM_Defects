\section{Proposed PCAS Method}
\label{sec:method}

As discussed earlier in Section~\ref{sec:intro}, we aim to construct a surrogate model for mapping the set of inputs to a
field quantity. In the proposed methodology, we outline a two-step process to accomplish this. The first step involves
dimension reduction in the output space that involves
identification of principal directions or components in the dataset for the field of interest. These components are used
to construct a feature vector. Each element in the feature vector is an inner product of the field data and elements
of a given principal direction. Hence, the number of features is equal to the number of principal directions used to
reconstruct the field. In the second step, each feature is represented as a function of the inputs and a low-dimensional
representation of the function is computed
using the active subspace methodology outlined in~\cite{Constantine:2015}. 
To avoid unnecessary computations of the finite element model and thereby enhance the efficiency of the proposed method,
the two steps are implemented in an iterative manner as discussed further below. 
In~\ref{sub:pca}, we outline
the strategy for computing the feature vector. In~\ref{sub:as}, we provide a brief background on active subspaces
and outline the sequence of steps for surrogate construction for each feature. The overall iterative procedure for
constructing the surrogate is discussed in~\ref{sub:pcas}.

\subsection{Output Dimension Reduction using PCA}
\label{sub:pca}

For the purpose of outlining the computational framework, we consider a field,
$\mat{S}(\bm{\theta})\in\mathbb{R}^{r\times c}$ evaluated on a 2-dimensional mesh of size ($r\times c$)
for a given set of inputs $\bm{\theta}$. Consider that the field data is available at $N_s$ pseudorandom
samples, drawn from the joint probability density function (PDF) of $\bm{\theta}$. A data matrix $\mat{X}$ is
first constructed using the field data at $N_s$ samples. A singular value decomposition of the covariance
matrix, $\mat{X^\top X}$ is then performed to obtain an orthogonal matrix, $\mat{U}$ whose columns 
contain eigenvectors or principal directions in the considered data. A matrix, $\mathcal{Z}$ with 
rows as feature vectors corresponding to each
sample is obtained by multiplying the matrices, $\mat{X}$ and $\mat{U}$. The number of features
are thus equal to the number of components or columns considered in $\mat{U}$. 
The field $\mat{S}$ can be reconstructed by multiplying $\mathcal{Z}$ and the transpose of $\mat{U}$
considering the latter is orthogonal. 
Since most of the information is captured by the dominant eigenvectors, $\mat{S}$ can be re-constructed with a
reasonable amount of accuracy in a low-dimensional column space of $\mat{U}$. We adopt an iterative
approach wherein the number of eigenvectors or components of $\mat{U}$ are increased by one at each iteration
and the accuracy of the reconstructed field $\hat{\mat S}$ is assessed. Thus, the optimal number of components
correspond to the reconstruction error~$\varepsilon_\mathcal{R}^\infty$ 
being smaller than a considered threshold $\tau$. The sequence of steps
is outlined in Algorithm~\ref{alg:pca}.
%
\bigskip
\begin{spacing}{0.95}
\begin{breakablealgorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
  \caption{Determining the optimal number of components, $K^\ast$ for reconstructing $\mat{S}$}
  \begin{algorithmic}[1]
  \Require $\tau$, $\mat{S}_i$'s, $\bm{\theta_i}$'s ($i=1,2,\ldots,N_s$)
  \Ensure $\mat{U}^r$, $K^\ast$
	\Procedure{Output Dimension Reduction}{} 
	\State Construct the data matrix, $\mat X$:
	\State Set $k$ = 0
	\Loop
	  \State Reshape $\mat{S}_i\in\mathbb{R}^{r\times c}$ into a vector,
	  $\vec{S}_{v,i}\in\mathbb{R}^{(r\ast c)\times 1}$
          \State $k=k+1$
	  \State $\mat X(k,:)=\vec{S}_{v,i}$
	  \If {$k=N_s$}
			\State break
		\EndIf
	\EndLoop
	\State Perform an SVD on the covariance matrix, $\mat{X}^\top \mat{X}$:
	\Statex \[ \mat{X}^\top \mat{X} = \mat{U}\mat{S}\mat{V}^\top \]
	\State Optimize the number of components, $K$:
	\State Set $K$ = 1
	\Loop
	 \State Compute the feature matrix, $\mat{\mathcal{Z}}$:
	  \[ \mat{\mathcal{Z}} = \mat{X}\mat{U}(:,1:K) \]
	  \State Reconstruct the field, $\mat{S}$ as $\hat{\mat{S}}$:
	  \[ \hat{\mat{S}} = \mat{\mathcal{Z}}\mat{U}(:,1:K)^\top \]
	  \State Estimate the maximum error, $\varepsilon_\mathcal{R}^\infty$:
	  \[ \varepsilon_\mathcal{R}^\infty = \max\limits_i \|\mat{S}_i-\hat{\mat{S}}_i\|_\infty,~i=1,2,\ldots,N_s\]
	   \If {$\varepsilon^\infty < \tau$}
	                \State $K^\ast = K$
	                \State $\mat{U}^r = \mat{U}(:,1:K^\ast)$
			\State break
		\EndIf
          \State $K=K+1$
	\EndLoop
	\EndProcedure
  \end{algorithmic}
  \label{alg:pca}
\end{breakablealgorithm}
\end{spacing}
\bigskip
%

At the end of the iterative procedure, a feature vector with $K^\ast$ components is obtained for each $\bm{\theta_i}$.
Dimension reduction in the output space is thus achieved since $K^\ast\ll (r\ast c)$, where $(r\ast c)$ is the 
dimensionality of the column space of $\mat{U}$.
The feature matrix, $\mat{\mathcal{Z}}$ can be mathematically represented as follows:

\be
\mat{\mathcal{Z}} = 
\begin{pmatrix}
\mathcal{Z}_{11} & \mathcal{Z}_{21} & \cdots & \mathcal{Z}_{K^\ast 1} \\
\mathcal{Z}_{12} & \mathcal{Z}_{22}  & \cdots & \mathcal{Z}_{K^\ast 2} \\
\vdots & \vdots & \ddots & \vdots \\
\mathcal{Z}_{1N_s} & \mathcal{Z}_{2N_s} & \cdots & \mathcal{Z}_{K^\ast N_s} 
\end{pmatrix}
\label{eq:feature}
\ee
%
The data matrix in the RHS of~\eqref{eq:feature} is used to construct an active subspace for each feature, 
$\mathcal{Z}_{i}$~($i$~=~$1,2,\ldots,K^\ast$) as discussed in the following section.

\subsection{Active Subspace Discovery}
\label{sub:as}

A given feature, $\mathcal{Z}_{i}$ = $\mathcal{Z}_{i}(\vec{\theta})$ can be considered as a scalar valued function
of the set of inputs, $\bm{\theta}$. An active subspace in the present context is a low-dimensional subspace in the input 
domain that effectively captures the variability in $\mathcal{Z}_{i}$ due to variations in $\bm{\theta}$. 
The set of inputs, $\bm{\theta}$ in the physical space
are parameterized as canonical random variables, $\bm{\xi}\in\Omega\in\mathbb{R}^{\Nt}$, where $N_\theta$
denotes the number of uncertain parameters referred to as the dimensionality of the parameter space. The active
subspace is spanned by the dominant eigenvectors of a matrix, $\mathbb{C}$ comprising the derivative information
of $\mathcal{Z}_{i}$ with respect to the components of $\bm{\xi}$. Note that a component $\xi_k$ can be projected back
to the physical space to its corresponding potential parameter, $\theta_k$. The positive semi-definite matrix,
 $\mathbb{C}$ for the $i^\text{th}$ feature is given as follows:
%
\be
\mathbb{C}_i = \int_\Omega (\nabla_{\vec{\xi}}\mathcal{Z}_{i})(\nabla_{\vec{\xi}}\mathcal{Z}_{i})^\top dP_\vec\xi, 
\label{eq:C}
\ee
%
where $dP_\vec\xi$ = $\pi_\vec\xi d\vec\xi$  and $\pi_\vec\xi$ denotes the joint PDF of $\bm{\xi}$. Note that
$\mathcal{Z}_{i}$ is assumed to be differentiable and L$^2$ integrable in $\Omega_\theta$. 
Since the integral in~\eqref{eq:C}
is multidimensional, the symmetric and positive semidefinite matrix $\mathbb{C}_i$ is approximated numerically in 
practice. Consider its sampling based estimate and associated eigenvalue decomposition as follows:
%
 \be
 \mathbb{C}_i\approx \hat{\mathbb{C}}_i = \frac{1}{N}\sum\limits_{l=1}^{N} 
 (\nabla_{\vec{\xi}}\mathcal{Z}_{i}(\vec{\xi}_l))(\nabla_{\vec{\xi}}\mathcal{Z}_{i}(\vec{\xi}_l))^\top
 = \hat{\mat{W}}\hat{\mat{\Lambda}}\hat{\mat{W}}^\top.
\label{eq:chat}
 \ee
 %
 The matrix $\hat{\mat{W}}$ comprises orthonormal eigenvectors as its columns, and $\hat{\mat{\Lambda}}$
 is a diagonal matrix with eigenvalues arranged in descending order as its elements:
 \[
     \lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_\Nt \geq 0.
\] 
Dimension reduction is achieved by partitioning the eigenpairs about the $j^{\text{th}}$ eigenvalue
such that  \scalebox{1.25}{$\left(\frac{\lambda_j}{\lambda_{j+1}}\right)$}$\gg 1$ as follows:
\be
 \hat{\mat{W}} = [\hat{\mat{W}}_1~\hat{\mat{W}}_2],~~\hat{\mat{\Lambda}} = \begin{bmatrix}\hat{\mat{\Lambda}}_1 & \\  &
  \hat{\mat{\Lambda}}_2. 
\end{bmatrix}
\ee
%
The column space of $\hat{\mat{W}}_1$ constitutes the active subspace, and $\hat{\mat{\Lambda}}_1$ is the 
corresponding diagonal matrix with its elements: $\{\lambda_1,\ldots,\lambda_\Nj\}$, where $\Nj$ is the number
of columns or eigenvectors in $\hat{\mat{W}}_1$. $\mathcal{Z}_{i}$,
a function of $\Nt$ independent variables is transformed as $G(\bm{\eta})$, a function of $j$ independent
variables since $\bm{\eta}=\hat{\mat{W}}_1^\top \bm{\xi}\in\mathbb{R}^\Nj$. The components of $\bm{\eta}$
are referred to as \textit{active variables}.
From~\eqref{eq:chat}, it is clear that the computational effort needed to construct $\mathbb{C}_i$ is directly proportional 
to the number of samples, $N$. A regression-based approach outlined
in~\cite{Vohra:2019} is used to estimate the gradients of $\mathcal{Z}_{i}$,
required to compute the elements of $\mathbb{C}_i$.

\subsubsection{Surrogate in the Active subspace}
\label{sub:surr}

Significant computational gains are expected in situations where a low-dimensional active subspace captures the
variability in the output with reasonable accuracy. However, computational gains as a result of dimension reduction
can be enhanced by approximating the variability~$G(\bm{\eta})$ in the subspace with a surrogate
model~$\hat{G}(\bm{\eta})$. In situations where the variability is captured using 1-2 dominant eigenvectors in
the active subspace, a simple polynomial regression fit is a reasonable choice for the surrogate model. 
However, for a relatively large dimensional surrogate model, one could
use a PCE or a GP. It is critical to validate the surrogate model for its predictive accuracy. 
The following algorithm provides a sequence of steps adapted from~\cite{Constantine:2015} to construct a
surrogate model in the active subspace.
%
\begin{breakablealgorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
  \caption{For constructing a surrogate model in the active subspace}
  \begin{algorithmic}[1]
	\Procedure{Surrogate Model, $\hat{G}(\bm{\eta})$}{} 
	  \State Consider $N$ available data points in the full space, $(\vec\xi_k,\mathcal{Z}_i(\vec\xi_k))$, $k~=~1,\ldots,N$
	  \State For each $\vec\xi_k$, compute $\vec\eta_k$ = $\mat{W}_1^\top\vec\xi_k$ 
          (Note: $G(\vec{\eta}_k)$ $\approx$ $\mathcal{Z}_i(\vec{\xi}_k)$)
	  \State Fit a regression surface, $\hat{G}(\bm{\eta})$ to approximate $G(\bm{\eta})$ using the data
                 points, $(\vec\xi_k,G(\vec\eta_k))$
	  \State Note that the overall approximation is: $\mathcal{Z}_i(\vec{\xi})$ $\approx$
                 $\hat{G}(\mat{W}_1^\top\vec{\xi})$ 
	\EndProcedure
  \end{algorithmic}
  \label{alg:surr}
\end{breakablealgorithm} 
%

\bigskip
To sum up, an active subspace is computed for each dominant feature, $\mathcal{Z}_i$ and a corresponding
surrogate fit, $\hat{\mathcal{Z}}_i$ is performed. Therefore, a total of $K^\ast$ surrogate models are constructed
to map the set of inputs $\bm{\theta}$ in the physical space to the field in the output space. 
Therefore, at the end of the step process, dimension reduction in the output space is 
$\mathbb{R}^{(r\ast c)}\rightarrow \mathbb{R}^{K^\ast}$, and dimension reduction in the input space
is $\mathbb{R}^{K^\ast}\rightarrow \mathbb{R}^{N_{\bm{\eta},\max}}$; 
where $N_{\bm{\eta},\max}$ corresponds to the surrogate model with the largest dimensionality in $\bm{\eta}$.
The iterative procedure for constructing the surrogate model is
discussed in~\ref{sub:pcas}.

\subsection{Iterative Procedure for Surrogate Construction}
\label{sub:pcas}

As a first step, we construct a cross-validation dataset using the FEM
 for testing the accuracy of the surrogate model. 
An initial set realizations of the residual stress field is generated at $N_0$ samples 
(drawn from the joint PDF of $\bm{\theta}$) using the FEM. 
The optimal number of components~($K^0$) or representative features~($\mathcal{Z}_i^0$, $i=1,\ldots,K^0$)
are then determined for
the initial dataset using Algorithm~\ref{alg:pca}. A map from $\bm{\xi}$ to each feature $\mathcal{Z}_i^0$
is approximated by a surrogate model: $\mathcal{Z}_i^0(\bm{\xi})\approx 
\hat{G}_i(\bm{\eta})$ in the active subspace using
the methodology presented in~\ref{sub:as}. The residual field is reconstructed using surrogate 
prediction for each feature $\mathcal{Z}_i^0$. Finally, we compute verification error~($\varepsilon_0^{\text{ver}}$)
and validation error~($\varepsilon_0^{\text{val}}$)
for the surrogate model by reconstructing the field at the same set of $N_0$
samples used for building the surrogate model and the samples used for cross-validation respectively. 
The mathematical expression for the two errors is given as
follows:
%
\be
\varepsilon_0^{\text{ver,val}} = 
\frac{1}{N_0^{\text{ver,val}}}\sum\limits_{i=1}^{N_0^{\text{ver,val}}} \frac{\|\mat{S}_0-\hat{\mat{S}}_0\|_2}{\|\mat{S}_0\|_2},
\label{eq:check}
\ee
%
where $N_0^{\text{ver}}$ and $N_0^{\text{val}}$ denote the number of samples used for verification and validation
respectively; $\mat{S}_0$ and $\hat{\mat{S}}_0$ denote the stress field simulated using the FEM and the 
surrogate model respectively. Note that the subscript `0' indicates that these variables correspond to the initial set
of computations. 
A new set of realizations for the residual stress field is generated at each subsequent iteration
and augmented with the existing dataset. The two step process is repeated until both $\varepsilon_0^{\text{ver}}$
and $\varepsilon_0^{\text{val}}$ are found to be smaller than 0.1. In other words, convergence is established 
once the reconstructed field is observed to be accurate within 10$\%$. 
The sequence of steps associated with the iterative procedure for building the surrogate model is provided in
Algorithm~\ref{alg:pcas}. 
%
%\begin{spacing}{0.95}
\begin{breakablealgorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
  \caption{Iterative strategy for surrogate modeling using the PCAS method}
  \begin{algorithmic}[1]
  \Require Nominal values and intervals for each component of $\bm{\theta}$, Error threshold:~$\tau$
  \Ensure $K^\ast$, $\hat{G}_i^\ast(\bm{\eta})$~($i=1,\ldots,K^\ast$)
  \Procedure{PCAS Methodology}{} 
    \State Draw an initial set of $N_0$ samples from the joint PDF $\pi_{\bm{\theta}}$
    \State Generate FEM realizations of the residual stress field at these $N_0$ samples
    \State Determine optimal number of components $K^0$ using Algorithm~\ref{alg:pca}
    \State Discover an active subspace for each feature $\mathcal{Z}_i^0(\bm{\theta})$
    \State Approximate $\mathcal{Z}_i^0(\bm{\theta})$ with a low-dimensional surrogate $\hat{G}_i^0(\bm{\eta})$ 
    \State Reconstruct the residual stress field $\hat{\mat{S}}_0$
    \State Evaluate the errors: $\varepsilon_0^{\text{ver}}$ and $\varepsilon_0^{\text{val}}$ using~\eqref{eq:check}
    \If {($\varepsilon_0^{\text{ver}}<\tau$ AND $\varepsilon_0^{\text{val}}<\tau$)}
    \State $K^\ast=K^0$
    \State $\hat{G}_i^\ast(\bm{\eta})=\hat{G}_i^0(\bm{\eta})$
    \State Proceed to Step 17
    \EndIf
    \State Draw a new set of samples $\bm{\theta}^{\text{new}}$ from $\pi_{\bm{\theta}}$ and 
    perform model evaluations at $\bm{\theta}^{\text{new}}$
    \State Augment the new dataset with existing dataset for building the surrogate
    \State Repeat Steps 4--9 and 14--15 until the `if' condition is satisfied
  \EndProcedure
  \end{algorithmic}
  \label{alg:pcas}
\end{breakablealgorithm} 
%\end{spacing}

An overall flow diagram for the two
step process implemented at each iteration for the input and output dimension reduction is illustrated in
Figure~\ref{fig:fd}. Once dimension reduction for a given iteration is complete, the field is reconstructed
using the sequence illustrated in Figure~\ref{fig:re}.
%
\begin{figure}[htbp]
\begin{center}
\begin{tikzpicture}[node distance=1.2cm,scale=0.5, every node/.style={scale=0.8}]

\node (field) [io, text width=6em] {Field Data\\ $\mat{S}\in\mathbb{R}^N$};

\node (pca) [process, right of=field, text width=15em, xshift=8.2cm] {Optimal number of features: \\ 
$\mathcal{Z}_i$, $i=1,\ldots,K^\ast$\\
DR: $\mathbb{R}^{N}\rightarrow \mathbb{R}^{K^\ast},~K^\ast\ll N$};

\draw [arrow] (field) -- node[above] {Principal Component} node [below] {Analysis} (pca);

\node (zk) [process, below of=pca, text width=8em, yshift=-3.0cm] {$\mathcal{Z}_i=\mathcal{Z}_i(\bm{\theta})$\\
$\bm{\theta}\in\mathbb{R}^{\Nt}$};

\draw [arrow] (pca) -- (zk);

\node (as) [io, below of=field, text width=16em, yshift=-3.0cm]{$\mathcal{Z}_i(\bm{\theta})\approx 
G(\bm{\eta}=\bm{\theta}^\top \bm{W_1})$\\ 
$G(\bm{\eta})\approx \hat{G}(\bm{\eta})$, $\eta\in\mathbb{R}^{\Nj}$\\
DR: $\mathbb{R}^{\Nt}\rightarrow \mathbb{R}^{N_{\bm{\eta},\max}}$, $N_{\bm{\eta},\max}\ll \Nt$};

\draw [arrow] (zk) -- node [above] {Active Subspace} node [below] {Computation} (as);

\draw [arrow,dashed] (as) -- (field);

\end{tikzpicture}
\end{center}
\caption{Flow diagram illustrating the sequence of steps and associated dimension reduction (DR) in the PCAS method.}
\label{fig:fd}
\end{figure}
%
\begin{figure}[htbp]
\begin{center}
\begin{tikzpicture}[node distance=1.2cm,scale=0.6, every node/.style={scale=0.8}]

\node (s1) [io, text width=7.5em] {Draw a sample, $\bm{\xi}_k$ from $\pi_\xi$};

\node (s2) [process, right of=s1, text width=8em, xshift=3.0cm] {Compute $\hat{G}_i(\bm{\xi}_k)$\\ $i=1,2,\ldots,K^\ast$};

\draw [arrow] (s1) -- (s2);

\node (s3) [process, right of=s2, text width=9em, xshift=3.0cm] {$\mathcal{Z}_i(\bm{\xi}_k)\approx\hat{G}_i(\bm{\xi}_k)$};

\draw [arrow] (s2) -- (s3);

\node (s4) [process, right of=s3, text width=7em, xshift=2.8cm] {Compute $\hat{\mat{S}}(\mathcal{Z}_i)$};

\draw [arrow] (s3) -- (s4);

\end{tikzpicture}
\end{center}
\caption{Flow diagram illustrating the sequence of steps for reconstructing the field of interest.}
\label{fig:re}
\end{figure}
\bigskip

%









